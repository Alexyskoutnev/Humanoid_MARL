{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing mediapy:\n"
     ]
    }
   ],
   "source": [
    "#@title Import packages for plotting and creating graphics\n",
    "import time\n",
    "import itertools\n",
    "import numpy as np\n",
    "from typing import Callable, NamedTuple, Optional, Union, List\n",
    "\n",
    "# Graphics and plotting.\n",
    "print('Installing mediapy:')\n",
    "!command -v ffmpeg >/dev/null || (apt update && apt install -y ffmpeg)\n",
    "!pip install -q mediapy\n",
    "import mediapy as media\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# More legible printing from numpy.\n",
    "np.set_printoptions(precision=3, suppress=True, linewidth=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Import MuJoCo, MJX, and Brax\n",
    "\n",
    "from datetime import datetime\n",
    "import functools\n",
    "import jax\n",
    "from jax import numpy as jp\n",
    "import numpy as np\n",
    "from typing import Any, Dict, Tuple, Union\n",
    "\n",
    "from brax import envs\n",
    "from brax import math\n",
    "from brax.base import Base, Motion, Transform\n",
    "from brax.envs.base import Env, State\n",
    "from brax.training.agents.ppo import train as ppo\n",
    "from brax.training.agents.ppo import networks as ppo_networks\n",
    "from brax.io import model\n",
    "from etils import epath\n",
    "from flax import struct\n",
    "from matplotlib import pyplot as plt\n",
    "import mediapy as media\n",
    "from ml_collections import config_dict\n",
    "import mujoco\n",
    "from mujoco import mjx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_gpu():\n",
    "    devices = jax.devices()\n",
    "    if len(devices) == 0:\n",
    "        print(\"No GPU devices found.\")\n",
    "    else:\n",
    "        print(f\"Found {len(devices)} GPU device(s):\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 GPU device(s):\n"
     ]
    }
   ],
   "source": [
    "check_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title MjxEnv\n",
    "\n",
    "class MjxEnv(Env):\n",
    "  \"\"\"API for driving an MJX system for training and inference in brax.\"\"\"\n",
    "\n",
    "  def __init__(\n",
    "      self,\n",
    "      mj_model: mujoco.MjModel,\n",
    "      physics_steps_per_control_step: int = 1,\n",
    "  ):\n",
    "    \"\"\"Initializes MjxEnv.\n",
    "\n",
    "    Args:\n",
    "      mj_model: mujoco.MjModel\n",
    "      physics_steps_per_control_step: the number of times to step the physics\n",
    "        pipeline for each environment step\n",
    "    \"\"\"\n",
    "    self.model = mj_model\n",
    "    self.data = mujoco.MjData(mj_model)\n",
    "    self.sys = mjx.device_put(mj_model)\n",
    "    self._physics_steps_per_control_step = physics_steps_per_control_step\n",
    "\n",
    "  def pipeline_init(\n",
    "      self, qpos: jax.Array, qvel: jax.Array\n",
    "  ) -> mjx.Data:\n",
    "    \"\"\"Initializes the physics state.\"\"\"\n",
    "    data = mjx.device_put(self.data)\n",
    "    data = data.replace(qpos=qpos, qvel=qvel, ctrl=jp.zeros(self.sys.nu))\n",
    "    data = mjx.forward(self.sys, data)\n",
    "    return data\n",
    "\n",
    "  def pipeline_step(\n",
    "      self, data: mjx.Data, ctrl: jax.Array\n",
    "  ) -> mjx.Data:\n",
    "    \"\"\"Takes a physics step using the physics pipeline.\"\"\"\n",
    "    def f(data, _):\n",
    "      data = data.replace(ctrl=ctrl)\n",
    "      return (\n",
    "          mjx.step(self.sys, data),\n",
    "          None,\n",
    "      )\n",
    "    data, _ = jax.lax.scan(f, data, (), self._physics_steps_per_control_step)\n",
    "    return data\n",
    "\n",
    "  @property\n",
    "  def dt(self) -> jax.Array:\n",
    "    \"\"\"The timestep used for each env step.\"\"\"\n",
    "    return self.sys.opt.timestep * self._physics_steps_per_control_step\n",
    "\n",
    "  @property\n",
    "  def observation_size(self) -> int:\n",
    "    rng = jax.random.PRNGKey(0)\n",
    "    reset_state = self.unwrapped.reset(rng)\n",
    "    return reset_state.obs.shape[-1]\n",
    "\n",
    "  @property\n",
    "  def action_size(self) -> int:\n",
    "    return self.sys.nu\n",
    "\n",
    "  @property\n",
    "  def backend(self) -> str:\n",
    "    return 'mjx'\n",
    "\n",
    "  def _pos_vel(\n",
    "      self, data: mjx.Data\n",
    "      ) -> Tuple[Transform, Motion]:\n",
    "    \"\"\"Returns 6d spatial transform and 6d velocity for all bodies.\"\"\"\n",
    "    x = Transform(pos=data.xpos[1:, :], rot=data.xquat[1:, :])\n",
    "    cvel = Motion(vel=data.cvel[1:, 3:], ang=data.cvel[1:, :3])\n",
    "    offset = data.xpos[1:, :] - data.subtree_com[\n",
    "        self.model.body_rootid[np.arange(1, self.model.nbody)]]\n",
    "    xd = Transform.create(pos=offset).vmap().do(cvel)\n",
    "    return x, xd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Humanoid Enviroment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Humanoid(MjxEnv):\n",
    "\n",
    "  def __init__(\n",
    "      self,\n",
    "      forward_reward_weight=1.25,\n",
    "      ctrl_cost_weight=0.1,\n",
    "      healthy_reward=5.0,\n",
    "      terminate_when_unhealthy=True,\n",
    "      healthy_z_range=(1.0, 2.0),\n",
    "      reset_noise_scale=1e-2,\n",
    "      exclude_current_positions_from_observation=True,\n",
    "      **kwargs,\n",
    "  ):\n",
    "    path = epath.Path(epath.resource_path('mujoco')) / (\n",
    "        'mjx/benchmark/model/humanoid'\n",
    "    )\n",
    "    mj_model = mujoco.MjModel.from_xml_path(\n",
    "        (path / 'humanoid.xml').as_posix())\n",
    "    mj_model.opt.solver = mujoco.mjtSolver.mjSOL_CG\n",
    "    mj_model.opt.iterations = 6\n",
    "    mj_model.opt.ls_iterations = 6\n",
    "\n",
    "    physics_steps_per_control_step = 5\n",
    "    kwargs['physics_steps_per_control_step'] = kwargs.get(\n",
    "        'physics_steps_per_control_step', physics_steps_per_control_step)\n",
    "\n",
    "    super().__init__(mj_model=mj_model, **kwargs)\n",
    "\n",
    "    self._forward_reward_weight = forward_reward_weight\n",
    "    self._ctrl_cost_weight = ctrl_cost_weight\n",
    "    self._healthy_reward = healthy_reward\n",
    "    self._terminate_when_unhealthy = terminate_when_unhealthy\n",
    "    self._healthy_z_range = healthy_z_range\n",
    "    self._reset_noise_scale = reset_noise_scale\n",
    "    self._exclude_current_positions_from_observation = (\n",
    "        exclude_current_positions_from_observation\n",
    "    )\n",
    "\n",
    "  def reset(self, rng: jp.ndarray) -> State:\n",
    "    \"\"\"Resets the environment to an initial state.\"\"\"\n",
    "    rng, rng1, rng2 = jax.random.split(rng, 3)\n",
    "\n",
    "    low, hi = -self._reset_noise_scale, self._reset_noise_scale\n",
    "    qpos = self.sys.qpos0 + jax.random.uniform(\n",
    "        rng1, (self.sys.nq,), minval=low, maxval=hi\n",
    "    )\n",
    "    qvel = jax.random.uniform(\n",
    "        rng2, (self.sys.nv,), minval=low, maxval=hi\n",
    "    )\n",
    "\n",
    "    data = self.pipeline_init(qpos, qvel)\n",
    "\n",
    "    obs = self._get_obs(data, jp.zeros(self.sys.nu))\n",
    "    reward, done, zero = jp.zeros(3)\n",
    "    metrics = {\n",
    "        'forward_reward': zero,\n",
    "        'reward_linvel': zero,\n",
    "        'reward_quadctrl': zero,\n",
    "        'reward_alive': zero,\n",
    "        'x_position': zero,\n",
    "        'y_position': zero,\n",
    "        'distance_from_origin': zero,\n",
    "        'x_velocity': zero,\n",
    "        'y_velocity': zero,\n",
    "    }\n",
    "    return State(data, obs, reward, done, metrics)\n",
    "\n",
    "  def step(self, state: State, action: jp.ndarray) -> State:\n",
    "    \"\"\"Runs one timestep of the environment's dynamics.\"\"\"\n",
    "    data0 = state.pipeline_state\n",
    "    data = self.pipeline_step(data0, action)\n",
    "\n",
    "    com_before = data0.subtree_com[1]\n",
    "    com_after = data.subtree_com[1]\n",
    "    velocity = (com_after - com_before) / self.dt\n",
    "    forward_reward = self._forward_reward_weight * velocity[0]\n",
    "\n",
    "    min_z, max_z = self._healthy_z_range\n",
    "    is_healthy = jp.where(data.qpos[2] < min_z, x=0.0, y=1.0)\n",
    "    is_healthy = jp.where(\n",
    "        data.qpos[2] > max_z, x=0.0, y=is_healthy\n",
    "    )\n",
    "    if self._terminate_when_unhealthy:\n",
    "      healthy_reward = self._healthy_reward\n",
    "    else:\n",
    "      healthy_reward = self._healthy_reward * is_healthy\n",
    "\n",
    "    ctrl_cost = self._ctrl_cost_weight * jp.sum(jp.square(action))\n",
    "\n",
    "    obs = self._get_obs(data, action)\n",
    "    reward = forward_reward + healthy_reward - ctrl_cost\n",
    "    done = 1.0 - is_healthy if self._terminate_when_unhealthy else 0.0\n",
    "    state.metrics.update(\n",
    "        forward_reward=forward_reward,\n",
    "        reward_linvel=forward_reward,\n",
    "        reward_quadctrl=-ctrl_cost,\n",
    "        reward_alive=healthy_reward,\n",
    "        x_position=com_after[0],\n",
    "        y_position=com_after[1],\n",
    "        distance_from_origin=jp.linalg.norm(com_after),\n",
    "        x_velocity=velocity[0],\n",
    "        y_velocity=velocity[1],\n",
    "    )\n",
    "\n",
    "    return state.replace(\n",
    "        pipeline_state=data, obs=obs, reward=reward, done=done\n",
    "    )\n",
    "\n",
    "  def _get_obs(\n",
    "      self, data: mjx.Data, action: jp.ndarray\n",
    "  ) -> jp.ndarray:\n",
    "    \"\"\"Observes humanoid body position, velocities, and angles.\"\"\"\n",
    "    position = data.qpos\n",
    "    if self._exclude_current_positions_from_observation:\n",
    "      position = position[2:]\n",
    "\n",
    "    # external_contact_forces are excluded\n",
    "    return jp.concatenate([\n",
    "        position,\n",
    "        data.qvel,\n",
    "        data.cinert[1:].ravel(),\n",
    "        data.cvel[1:].ravel(),\n",
    "        data.qfrc_actuator,\n",
    "    ])\n",
    "\n",
    "\n",
    "envs.register_environment('humanoid', Humanoid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'envs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m env_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhumanoid\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[43menvs\u001b[49m\u001b[38;5;241m.\u001b[39mregister_environment(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhumanoid\u001b[39m\u001b[38;5;124m'\u001b[39m, Humanoid)\n\u001b[1;32m      3\u001b[0m env \u001b[38;5;241m=\u001b[39m envs\u001b[38;5;241m.\u001b[39mget_environment(env_name)\n\u001b[1;32m      4\u001b[0m train_fn \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(\n\u001b[1;32m      5\u001b[0m     ppo\u001b[38;5;241m.\u001b[39mtrain, num_timesteps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30_000_000\u001b[39m, num_evals\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, reward_scaling\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m,\n\u001b[1;32m      6\u001b[0m     episode_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, normalize_observations\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, action_repeat\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m      7\u001b[0m     unroll_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, num_minibatches\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, num_updates_per_batch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m,\n\u001b[1;32m      8\u001b[0m     discounting\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.97\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3e-4\u001b[39m, entropy_cost\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m, num_envs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2048\u001b[39m,\n\u001b[1;32m      9\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'envs' is not defined"
     ]
    }
   ],
   "source": [
    "env_name = 'humanoid'\n",
    "envs.register_environment('humanoid', Humanoid)\n",
    "env = envs.get_environment(env_name)\n",
    "train_fn = functools.partial(\n",
    "    ppo.train, num_timesteps=30_000_000, num_evals=5, reward_scaling=0.1,\n",
    "    episode_length=1000, normalize_observations=True, action_repeat=1,\n",
    "    unroll_length=10, num_minibatches=32, num_updates_per_batch=8,\n",
    "    discounting=0.97, learning_rate=3e-4, entropy_cost=1e-3, num_envs=2048,\n",
    "    batch_size=1024, seed=0)\n",
    "make_inference_fn, params, _= train_fn(environment=env, progress_fn=None)\n",
    "\n",
    "model_path = './models/humandoid_policy'\n",
    "params = model.load_params(model_path)\n",
    "print(params)\n",
    "# inference_fn = make_inference_fn(params)\n",
    "# jit_inference_fn = jax.jit(inference_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the Model\n",
    "Here we will look at the locomotion policy that was trained on the humanoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-08 14:42:30.031846: W external/xla/xla/service/gpu/nvptx_compiler.cc:698] The NVIDIA driver's CUDA version is 12.2 which is older than the ptxas CUDA version (12.3.103). Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n"
     ]
    }
   ],
   "source": [
    "# instantiate the environment\n",
    "eval_env = envs.get_environment(env_name)\n",
    "jit_reset = jax.jit(eval_env.reset)\n",
    "jit_step = jax.jit(eval_env.step)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "humanoid",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
