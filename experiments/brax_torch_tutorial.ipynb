{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Import Brax and some helper modules\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import collections\n",
    "from datetime import datetime\n",
    "import functools\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "from typing import Any, Callable, Dict, Optional, Sequence\n",
    "\n",
    "try:\n",
    "  import brax\n",
    "except ImportError:\n",
    "  !pip install git+https://github.com/google/brax.git@main\n",
    "  clear_output()\n",
    "  import brax\n",
    "\n",
    "from brax import envs\n",
    "from brax.envs.wrappers import gym as gym_wrapper\n",
    "from brax.envs.wrappers import torch as torch_wrapper\n",
    "from brax.io import metrics\n",
    "from brax.training.agents.ppo import train as ppo\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Up A PPO Based Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(nn.Module):\n",
    "  \"\"\"Standard PPO Agent with GAE and observation normalization.\"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               policy_layers: Sequence[int],\n",
    "               value_layers: Sequence[int],\n",
    "               entropy_cost: float,\n",
    "               discounting: float,\n",
    "               reward_scaling: float,\n",
    "               device: str = \"cuda\"):\n",
    "    super(Agent, self).__init__()\n",
    "\n",
    "    policy = []\n",
    "    for w1, w2 in zip(policy_layers, policy_layers[1:]):\n",
    "      policy.append(nn.Linear(w1, w2))\n",
    "      policy.append(nn.SiLU())\n",
    "    policy.pop()  # drop the final activation\n",
    "    self.policy = nn.Sequential(*policy)\n",
    "\n",
    "    value = []\n",
    "    for w1, w2 in zip(value_layers, value_layers[1:]):\n",
    "      value.append(nn.Linear(w1, w2))\n",
    "      value.append(nn.SiLU())\n",
    "    value.pop()  # drop the final activation\n",
    "    self.value = nn.Sequential(*value)\n",
    "\n",
    "    self.num_steps = torch.zeros((), device=device)\n",
    "    self.running_mean = torch.zeros(policy_layers[0], device=device)\n",
    "    self.running_variance = torch.zeros(policy_layers[0], device=device)\n",
    "\n",
    "    self.entropy_cost = entropy_cost\n",
    "    self.discounting = discounting\n",
    "    self.reward_scaling = reward_scaling\n",
    "    self.lambda_ = 0.95\n",
    "    self.epsilon = 0.3\n",
    "    self.device = device\n",
    "\n",
    "  @torch.jit.export\n",
    "  def dist_create(self, logits):\n",
    "    \"\"\"Normal followed by tanh.\n",
    "\n",
    "    torch.distribution doesn't work with torch.jit, so we roll our own.\"\"\"\n",
    "    loc, scale = torch.split(logits, logits.shape[-1] // 2, dim=-1)\n",
    "    scale = F.softplus(scale) + .001\n",
    "    return loc, scale\n",
    "\n",
    "  @torch.jit.export\n",
    "  def dist_sample_no_postprocess(self, loc, scale):\n",
    "    return torch.normal(loc, scale)\n",
    "\n",
    "  @classmethod\n",
    "  def dist_postprocess(cls, x):\n",
    "    return torch.tanh(x).to(cls.device)\n",
    "\n",
    "  @torch.jit.export\n",
    "  def dist_entropy(self, loc, scale):\n",
    "    log_normalized = 0.5 * math.log(2 * math.pi) + torch.log(scale)\n",
    "    entropy = 0.5 + log_normalized\n",
    "    entropy = entropy * torch.ones_like(loc)\n",
    "    dist = torch.normal(loc, scale)\n",
    "    log_det_jacobian = 2 * (math.log(2) - dist - F.softplus(-2 * dist))\n",
    "    entropy = entropy + log_det_jacobian\n",
    "    return entropy.sum(dim=-1)\n",
    "\n",
    "  @torch.jit.export\n",
    "  def dist_log_prob(self, loc, scale, dist):\n",
    "    log_unnormalized = -0.5 * ((dist - loc) / scale).square()\n",
    "    log_normalized = 0.5 * math.log(2 * math.pi) + torch.log(scale)\n",
    "    log_det_jacobian = 2 * (math.log(2) - dist - F.softplus(-2 * dist))\n",
    "    log_prob = log_unnormalized - log_normalized - log_det_jacobian\n",
    "    return log_prob.sum(dim=-1)\n",
    "\n",
    "  @torch.jit.export\n",
    "  def update_normalization(self, observation):\n",
    "    self.num_steps += observation.shape[0] * observation.shape[1]\n",
    "    input_to_old_mean = observation - self.running_mean\n",
    "    mean_diff = torch.sum(input_to_old_mean / self.num_steps, dim=(0, 1))\n",
    "    self.running_mean = self.running_mean + mean_diff\n",
    "    input_to_new_mean = observation - self.running_mean\n",
    "    var_diff = torch.sum(input_to_new_mean * input_to_old_mean, dim=(0, 1))\n",
    "    self.running_variance = self.running_variance + var_diff\n",
    "\n",
    "  @torch.jit.export\n",
    "  def normalize(self, observation):\n",
    "    variance = self.running_variance / (self.num_steps + 1.0)\n",
    "    variance = torch.clip(variance, 1e-6, 1e6)\n",
    "    return ((observation - self.running_mean) / variance.sqrt()).clip(-5, 5)\n",
    "\n",
    "  @torch.jit.export\n",
    "  def get_logits_action(self, observation):\n",
    "    observation = self.normalize(observation)\n",
    "    logits = self.policy(observation)\n",
    "    loc, scale = self.dist_create(logits)\n",
    "    action = self.dist_sample_no_postprocess(loc, scale)\n",
    "    return logits, action\n",
    "\n",
    "  @torch.jit.export\n",
    "  def compute_gae(self, truncation, termination, reward, values,\n",
    "                  bootstrap_value):\n",
    "    truncation_mask = 1 - truncation\n",
    "    # Append bootstrapped value to get [v1, ..., v_t+1]\n",
    "    values_t_plus_1 = torch.cat(\n",
    "        [values[1:], torch.unsqueeze(bootstrap_value, 0)], dim=0)\n",
    "    deltas = reward + self.discounting * (\n",
    "        1 - termination) * values_t_plus_1 - values\n",
    "    deltas *= truncation_mask\n",
    "\n",
    "    acc = torch.zeros_like(bootstrap_value)\n",
    "    vs_minus_v_xs = torch.zeros_like(truncation_mask)\n",
    "\n",
    "    for ti in range(truncation_mask.shape[0]):\n",
    "      ti = truncation_mask.shape[0] - ti - 1\n",
    "      acc = deltas[ti] + self.discounting * (\n",
    "          1 - termination[ti]) * truncation_mask[ti] * self.lambda_ * acc\n",
    "      vs_minus_v_xs[ti] = acc\n",
    "\n",
    "    # Add V(x_s) to get v_s.\n",
    "    vs = vs_minus_v_xs + values\n",
    "    vs_t_plus_1 = torch.cat([vs[1:], torch.unsqueeze(bootstrap_value, 0)], 0)\n",
    "    advantages = (reward + self.discounting *\n",
    "                  (1 - termination) * vs_t_plus_1 - values) * truncation_mask\n",
    "    return vs, advantages\n",
    "\n",
    "  @torch.jit.export\n",
    "  def loss(self, td: Dict[str, torch.Tensor]):\n",
    "    observation = self.normalize(td['observation'])\n",
    "    policy_logits = self.policy(observation[:-1])\n",
    "    baseline = self.value(observation)\n",
    "    baseline = torch.squeeze(baseline, dim=-1)\n",
    "\n",
    "    # Use last baseline value (from the value function) to bootstrap.\n",
    "    bootstrap_value = baseline[-1]\n",
    "    baseline = baseline[:-1]\n",
    "    reward = td['reward'] * self.reward_scaling\n",
    "    termination = td['done'] * (1 - td['truncation'])\n",
    "\n",
    "    loc, scale = self.dist_create(td['logits'])\n",
    "    behaviour_action_log_probs = self.dist_log_prob(loc, scale, td['action'])\n",
    "    loc, scale = self.dist_create(policy_logits)\n",
    "    target_action_log_probs = self.dist_log_prob(loc, scale, td['action'])\n",
    "\n",
    "    with torch.no_grad():\n",
    "      vs, advantages = self.compute_gae(\n",
    "          truncation=td['truncation'],\n",
    "          termination=termination,\n",
    "          reward=reward,\n",
    "          values=baseline,\n",
    "          bootstrap_value=bootstrap_value)\n",
    "\n",
    "    rho_s = torch.exp(target_action_log_probs - behaviour_action_log_probs)\n",
    "    surrogate_loss1 = rho_s * advantages\n",
    "    surrogate_loss2 = rho_s.clip(1 - self.epsilon,\n",
    "                                 1 + self.epsilon) * advantages\n",
    "    policy_loss = -torch.mean(torch.minimum(surrogate_loss1, surrogate_loss2))\n",
    "\n",
    "    # Value function loss\n",
    "    v_error = vs - baseline\n",
    "    v_loss = torch.mean(v_error * v_error) * 0.5 * 0.5\n",
    "\n",
    "    # Entropy reward\n",
    "    entropy = torch.mean(self.dist_entropy(loc, scale))\n",
    "    entropy_loss = self.entropy_cost * -entropy\n",
    "\n",
    "    return policy_loss + v_loss + entropy_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "StepData = collections.namedtuple(\n",
    "    'StepData',\n",
    "    ('observation', 'logits', 'action', 'reward', 'done', 'truncation'))\n",
    "\n",
    "\n",
    "def sd_map(f: Callable[..., torch.Tensor], *sds) -> StepData:\n",
    "  \"\"\"Map a function over each field in StepData.\"\"\"\n",
    "  items = {}\n",
    "  keys = sds[0]._asdict().keys()\n",
    "  for k in keys:\n",
    "    items[k] = f(*[sd._asdict()[k] for sd in sds])\n",
    "  return StepData(**items)\n",
    "\n",
    "\n",
    "def eval_unroll(agent, env, length):\n",
    "  \"\"\"Return number of episodes and average reward for a single unroll.\"\"\"\n",
    "  observation = env.reset()\n",
    "  episodes = torch.zeros((), device=agent.device)\n",
    "  episode_reward = torch.zeros((), device=agent.device)\n",
    "  for _ in range(length):\n",
    "    _, action = agent.get_logits_action(observation)\n",
    "    observation, reward, done, _ = env.step(Agent.dist_postprocess(action))\n",
    "    episodes += torch.sum(done)\n",
    "    episode_reward += torch.sum(reward)\n",
    "  return episodes, episode_reward / episodes\n",
    "\n",
    "\n",
    "def train_unroll(agent, env, observation, num_unrolls, unroll_length):\n",
    "  \"\"\"Return step data over multple unrolls.\"\"\"\n",
    "  sd = StepData([], [], [], [], [], [])\n",
    "  for _ in range(num_unrolls):\n",
    "    one_unroll = StepData([observation], [], [], [], [], [])\n",
    "    for _ in range(unroll_length):\n",
    "      logits, action = agent.get_logits_action(observation)\n",
    "      observation, reward, done, info = env.step(Agent.dist_postprocess(action))\n",
    "      one_unroll.observation.append(observation)\n",
    "      one_unroll.logits.append(logits)\n",
    "      one_unroll.action.append(action)\n",
    "      one_unroll.reward.append(reward)\n",
    "      one_unroll.done.append(done)\n",
    "      one_unroll.truncation.append(info['truncation'])\n",
    "    one_unroll = sd_map(torch.stack, one_unroll)\n",
    "    sd = sd_map(lambda x, y: x + [y], sd, one_unroll)\n",
    "  td = sd_map(torch.stack, sd)\n",
    "  return observation, td\n",
    "\n",
    "\n",
    "def train(\n",
    "    env_name: str = 'ant',\n",
    "    num_envs: int = 2048,\n",
    "    episode_length: int = 1000,\n",
    "    device: str = 'cuda',\n",
    "    num_timesteps: int = 30_000_000,\n",
    "    eval_frequency: int = 10,\n",
    "    unroll_length: int = 5,\n",
    "    batch_size: int = 1024,\n",
    "    num_minibatches: int = 32,\n",
    "    num_update_epochs: int = 4,\n",
    "    reward_scaling: float = .1,\n",
    "    entropy_cost: float = 1e-2,\n",
    "    discounting: float = .97,\n",
    "    learning_rate: float = 3e-4,\n",
    "    progress_fn: Optional[Callable[[int, Dict[str, Any]], None]] = None,\n",
    "):\n",
    "  \"\"\"Trains a policy via PPO.\"\"\"\n",
    "  env = envs.create(env_name, batch_size=num_envs,\n",
    "                    episode_length=episode_length,\n",
    "                    backend='spring')\n",
    "  env = gym_wrapper.VectorGymWrapper(env)\n",
    "  # automatically convert between jax ndarrays and torch tensors:\n",
    "  env = torch_wrapper.TorchWrapper(env, device=device)\n",
    "\n",
    "  # env warmup\n",
    "  env.reset()\n",
    "  action = torch.zeros(env.action_space.shape).to(device)\n",
    "  env.step(action)\n",
    "\n",
    "  # create the agent\n",
    "  policy_layers = [\n",
    "      env.observation_space.shape[-1], 64, 64, env.action_space.shape[-1] * 2\n",
    "  ]\n",
    "  value_layers = [env.observation_space.shape[-1], 64, 64, 1]\n",
    "  agent = Agent(policy_layers, value_layers, entropy_cost, discounting,\n",
    "                reward_scaling, device)\n",
    "  agent = torch.jit.script(agent.to(device))\n",
    "  optimizer = optim.Adam(agent.parameters(), lr=learning_rate)\n",
    "\n",
    "  sps = 0\n",
    "  total_steps = 0\n",
    "  total_loss = 0\n",
    "  for eval_i in range(eval_frequency + 1):\n",
    "    if progress_fn:\n",
    "      t = time.time()\n",
    "      with torch.no_grad():\n",
    "        episode_count, episode_reward = eval_unroll(agent, env, episode_length)\n",
    "      duration = time.time() - t\n",
    "      # TODO: only count stats from completed episodes\n",
    "      episode_avg_length = env.num_envs * episode_length / episode_count\n",
    "      eval_sps = env.num_envs * episode_length / duration\n",
    "      progress = {\n",
    "          'eval/episode_reward': episode_reward,\n",
    "          'eval/completed_episodes': episode_count,\n",
    "          'eval/avg_episode_length': episode_avg_length,\n",
    "          'speed/sps': sps,\n",
    "          'speed/eval_sps': eval_sps,\n",
    "          'losses/total_loss': total_loss,\n",
    "      }\n",
    "      progress_fn(total_steps, progress)\n",
    "\n",
    "    if eval_i == eval_frequency:\n",
    "      break\n",
    "\n",
    "    observation = env.reset()\n",
    "    num_steps = batch_size * num_minibatches * unroll_length\n",
    "    num_epochs = num_timesteps // (num_steps * eval_frequency)\n",
    "    num_unrolls = batch_size * num_minibatches // env.num_envs\n",
    "    total_loss = 0\n",
    "    t = time.time()\n",
    "    for _ in range(num_epochs):\n",
    "      observation, td = train_unroll(agent, env, observation, num_unrolls,\n",
    "                                     unroll_length)\n",
    "\n",
    "      # make unroll first\n",
    "      def unroll_first(data):\n",
    "        data = data.swapaxes(0, 1)\n",
    "        return data.reshape([data.shape[0], -1] + list(data.shape[3:]))\n",
    "      td = sd_map(unroll_first, td)\n",
    "\n",
    "      # update normalization statistics\n",
    "      agent.update_normalization(td.observation)\n",
    "\n",
    "      for _ in range(num_update_epochs):\n",
    "        # shuffle and batch the data\n",
    "        with torch.no_grad():\n",
    "          permutation = torch.randperm(td.observation.shape[1], device=device)\n",
    "          def shuffle_batch(data):\n",
    "            data = data[:, permutation]\n",
    "            data = data.reshape([data.shape[0], num_minibatches, -1] +\n",
    "                                list(data.shape[2:]))\n",
    "            return data.swapaxes(0, 1)\n",
    "          epoch_td = sd_map(shuffle_batch, td)\n",
    "\n",
    "        for minibatch_i in range(num_minibatches):\n",
    "          td_minibatch = sd_map(lambda d: d[minibatch_i], epoch_td)\n",
    "          loss = agent.loss(td_minibatch._asdict())\n",
    "          optimizer.zero_grad()\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "          total_loss += loss\n",
    "\n",
    "    duration = time.time() - t\n",
    "    total_steps += num_epochs * num_steps\n",
    "    total_loss = total_loss / (num_epochs * num_update_epochs * num_minibatches)\n",
    "    sps = num_epochs * num_steps / duration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'Agent' has no attribute 'device'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m   plt\u001b[38;5;241m.\u001b[39mplot(xdata, ydata)\n\u001b[1;32m     25\u001b[0m   plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m---> 27\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprogress_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime to jit: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimes[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mtimes[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime to train: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimes[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mtimes[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[7], line 95\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(env_name, num_envs, episode_length, device, num_timesteps, eval_frequency, unroll_length, batch_size, num_minibatches, num_update_epochs, reward_scaling, entropy_cost, discounting, learning_rate, progress_fn)\u001b[0m\n\u001b[1;32m     93\u001b[0m t \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 95\u001b[0m   episode_count, episode_reward \u001b[38;5;241m=\u001b[39m \u001b[43meval_unroll\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisode_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m duration \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# TODO: only count stats from completed episodes\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 22\u001b[0m, in \u001b[0;36meval_unroll\u001b[0;34m(agent, env, length)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(length):\n\u001b[1;32m     21\u001b[0m   _, action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mget_logits_action(observation)\n\u001b[0;32m---> 22\u001b[0m   observation, reward, done, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(\u001b[43mAgent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdist_postprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     23\u001b[0m   episodes \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(done)\n\u001b[1;32m     24\u001b[0m   episode_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(reward)\n",
      "Cell \u001b[0;32mIn[6], line 53\u001b[0m, in \u001b[0;36mAgent.dist_postprocess\u001b[0;34m(cls, x)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdist_postprocess\u001b[39m(\u001b[38;5;28mcls\u001b[39m, x):\n\u001b[0;32m---> 53\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtanh(x)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'Agent' has no attribute 'device'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# temporary fix to cuda memory OOM\n",
    "# os.environ['XLA_PYTHON_CLIENT_ALLOCATOR'] = 'platform'\n",
    "# import os\n",
    "# os.environ['TF_XLA_FLAGS'] = '--tf_xla_enable_xla_devices'\n",
    "# os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"False\"\n",
    "\n",
    "xdata = []\n",
    "ydata = []\n",
    "eval_sps = []\n",
    "train_sps = []\n",
    "times = [datetime.now()]\n",
    "\n",
    "def progress(num_steps, metrics):\n",
    "  times.append(datetime.now())\n",
    "  xdata.append(num_steps)\n",
    "  ydata.append(metrics['eval/episode_reward'].cpu())\n",
    "  eval_sps.append(metrics['speed/eval_sps'])\n",
    "  train_sps.append(metrics['speed/sps'])\n",
    "  clear_output(wait=True)\n",
    "  plt.xlim([0, 30_000_000])\n",
    "  plt.ylim([0, 6000])\n",
    "  plt.xlabel('# environment steps')\n",
    "  plt.ylabel('reward per episode')\n",
    "  plt.plot(xdata, ydata)\n",
    "  plt.show()\n",
    "\n",
    "train(progress_fn=progress)\n",
    "\n",
    "print(f'time to jit: {times[1] - times[0]}')\n",
    "print(f'time to train: {times[-1] - times[1]}')\n",
    "print(f'eval steps/sec: {np.mean(eval_sps)}')\n",
    "print(f'train steps/sec: {np.mean(train_sps)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "humanoid",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
