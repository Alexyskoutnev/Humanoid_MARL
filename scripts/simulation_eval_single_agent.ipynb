{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Humandoid MARL\n",
    "from Humanoid_MARL import envs\n",
    "from Humanoid_MARL.envs.base_env import GymWrapper, VectorGymWrapper\n",
    "from Humanoid_MARL.utils.visual import save_video, save_rgb_image\n",
    "from Humanoid_MARL.utils.torch_utils import save_models, load_models\n",
    "from Humanoid_MARL.agent.ppo.train_torch import eval_unroll, get_agent_actions\n",
    "from Humanoid_MARL.agent.ppo.agent import Agent\n",
    "# from Humanoid_MARL.envs.torch_wrapper import TorchWrapper\n",
    "from brax.envs.wrappers import torch as torch_wrapper\n",
    "from IPython.display import HTML, clear_output\n",
    "from brax.io import html\n",
    "import jax\n",
    "from Humanoid_MARL import envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1.3982e+00,  9.9995e-01,  2.6097e-03,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00],\n",
       "         [ 1.3963e+00,  9.9997e-01,  2.5863e-03,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00],\n",
       "         [ 1.3955e+00,  9.9995e-01, -1.1547e-03,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00],\n",
       "         ...,\n",
       "         [ 1.3944e+00,  9.9996e-01, -2.8839e-03,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00],\n",
       "         [ 1.3954e+00,  9.9997e-01, -5.6955e-03,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00],\n",
       "         [ 1.4034e+00,  9.9997e-01,  9.0429e-05,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00]], device='cuda:0'),\n",
       " tensor([5.0108, 4.9926, 5.0156,  ..., 5.0083, 5.0038, 5.0057], device='cuda:0'),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'),\n",
       " {'distance_from_origin': tensor([0.9438, 0.9426, 0.9420,  ..., 0.9402, 0.9418, 0.9493], device='cuda:0'),\n",
       "  'first_obs': tensor([[ 1.3996e+00,  1.0097e+00,  2.6965e-03,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 1.3976e+00,  1.0032e+00,  2.6548e-03,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 1.3969e+00,  9.9565e-01, -1.1617e-03,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          ...,\n",
       "          [ 1.3959e+00,  1.0063e+00, -2.9691e-03,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 1.3966e+00,  9.9264e-01, -5.5888e-03,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 1.4048e+00,  9.9533e-01,  3.1512e-05,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]], device='cuda:0'),\n",
       "  'first_pipeline_state': None,\n",
       "  'forward_reward': tensor([ 0.0108, -0.0074,  0.0156,  ...,  0.0083,  0.0038,  0.0057],\n",
       "         device='cuda:0'),\n",
       "  'reward_alive': tensor([5., 5., 5.,  ..., 5., 5., 5.], device='cuda:0'),\n",
       "  'reward_linvel': tensor([ 0.0108, -0.0074,  0.0156,  ...,  0.0083,  0.0038,  0.0057],\n",
       "         device='cuda:0'),\n",
       "  'reward_quadctrl': tensor([-0., -0., -0.,  ..., -0., -0., -0.], device='cuda:0'),\n",
       "  'steps': tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0'),\n",
       "  'truncation': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'),\n",
       "  'x_position': tensor([0.0114, 0.0250, 0.0281,  ..., 0.0099, 0.0282, 0.0151], device='cuda:0'),\n",
       "  'x_velocity': tensor([ 0.0087, -0.0059,  0.0124,  ...,  0.0066,  0.0030,  0.0045],\n",
       "         device='cuda:0'),\n",
       "  'y_position': tensor([-0.0012, -0.0023,  0.0023,  ..., -0.0127, -0.0089, -0.0040],\n",
       "         device='cuda:0'),\n",
       "  'y_velocity': tensor([-0.0033, -0.0053, -0.0021,  ...,  0.0112, -0.0135,  0.0094],\n",
       "         device='cuda:0')})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_name = \"humanoid\"\n",
    "\n",
    "config = {\n",
    "            'num_timesteps': 150_000_000,\n",
    "            'eval_reward_limit' : 15_000,\n",
    "            'eval_frequency': 100,\n",
    "            'episode_length': 1000,\n",
    "            'unroll_length': 10,\n",
    "            'num_minibatches': 32,\n",
    "            'num_update_epochs': 8,\n",
    "            'discounting': 0.97,\n",
    "            'learning_rate': 3e-4,\n",
    "            'entropy_cost': 2e-3,\n",
    "            'num_envs': 2048,\n",
    "            'batch_size': 512,\n",
    "            'env_name': env_name,\n",
    "            'device' : 'cuda',\n",
    "            'device_idx' : 0,\n",
    "            'model_path' : \"../models/20240212_214953_ppo_humanoid.pt\",\n",
    "        }\n",
    "env = envs.create(\n",
    "        env_name,\n",
    "        batch_size=config['num_envs'],\n",
    "        episode_length=config['episode_length'],\n",
    "        backend=\"generalized\",\n",
    "        device_idx=0,\n",
    "    )\n",
    "env = VectorGymWrapper(env)\n",
    "env = torch_wrapper.TorchWrapper(env, device=config['device'])\n",
    "obs = env.reset()\n",
    "action = torch.zeros(\n",
    "        (env.action_space.shape[0], env.action_space.shape[1] * env.num_agents)\n",
    "    ).to(config['device'])\n",
    "env.step(action)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models loaded from ../models/20240212_214953_ppo_humanoid.pt\n"
     ]
    }
   ],
   "source": [
    "agents = load_models(config['model_path'], Agent, device=config['device'])\n",
    "agents = [torch.jit.script(agent.to(config['device'])) for agent in agents]\n",
    "# agents = [agent.eval() for agent in agents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 79629.0 reward: 85.00701904296875\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "                episode_count, episode_reward = eval_unroll(\n",
    "                    agents, env, config['episode_length'], config['device'], render=None\n",
    "                )\n",
    "print(f\"Episode {episode_count} reward: {episode_reward}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "humanoid",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
